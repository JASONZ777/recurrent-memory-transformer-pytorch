<img src="./rmt.png" width="450px"></img>

## Recurrent Memory Transformer - Pytorch (wip)

Implementation of <a href="https://arxiv.org/abs/2207.06881">Recurrent Memory Transformer</a> in Pytorch.

## Alternatives

- <a href="https://github.com/lucidrains/block-recurrent-transformer-pytorch">Block Recurrent Transformer</a>

- <a href="https://github.com/lucidrains/memformer">Memformer</a>

## Citations

```bibtex
@inproceedings{bulatov2022recurrent,
  title   = {Recurrent Memory Transformer},
  author  = {Aydar Bulatov and Yuri Kuratov and Mikhail Burtsev},
  booktitle = {Advances in Neural Information Processing Systems},
  editor  = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year    = {2022},
  url     = {https://openreview.net/forum?id=Uynr3iPhksa}
}
```

```bibtex
@misc{bulatov2023scaling,
  title   = {Scaling Transformer to 1M tokens and beyond with RMT}, 
  author  = {Aydar Bulatov and Yuri Kuratov and Mikhail S. Burtsev},
  year    = {2023},
  eprint  = {2304.11062},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL}
}
```
